'NoneType' object has no attribute 'cadam32bit_grad_fp32'
[('', PeftModelForSeq2SeqLM(
  (base_model): LoraModel(
    (model): T5ForConditionalGeneration(
      (shared): Embedding(32128, 1024)
      (encoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): SharedWeightTransformer(
                    (transformer_encoder_): Sequential(
                      (0): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                            )
                            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (1): Linear(in_features=1024, out_features=512, bias=True)
                      (2): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                            )
                            (linear1): Linear(in_features=512, out_features=1024, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=1024, out_features=512, bias=True)
                            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (3): Linear(in_features=512, out_features=256, bias=True)
                      (4): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                            )
                            (linear1): Linear(in_features=256, out_features=512, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=512, out_features=256, bias=True)
                            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (5): Linear(in_features=256, out_features=512, bias=True)
                      (6): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                            )
                            (linear1): Linear(in_features=512, out_features=1024, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=1024, out_features=512, bias=True)
                            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (7): Linear(in_features=512, out_features=1024, bias=True)
                      (8): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                            )
                            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                    )
                  )
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedActDense(
                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                  (wo): Linear(in_features=2816, out_features=1024, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1-23): 23 x T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): SharedWeightTransformer(
                    (transformer_encoder_): Sequential(
                      (0): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                            )
                            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (1): Linear(in_features=1024, out_features=512, bias=True)
                      (2): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                            )
                            (linear1): Linear(in_features=512, out_features=1024, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=1024, out_features=512, bias=True)
                            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (3): Linear(in_features=512, out_features=256, bias=True)
                      (4): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                            )
                            (linear1): Linear(in_features=256, out_features=512, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=512, out_features=256, bias=True)
                            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (5): Linear(in_features=256, out_features=512, bias=True)
                      (6): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                            )
                            (linear1): Linear(in_features=512, out_features=1024, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=1024, out_features=512, bias=True)
                            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                      (7): Linear(in_features=512, out_features=1024, bias=True)
                      (8): TransformerEncoder(
                        (layers): ModuleList(
                          (0): TransformerEncoderLayer(
                            (self_attn): MultiheadAttention(
                              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                            )
                            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                            (dropout): Dropout(p=0.1, inplace=False)
                            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                            (dropout1): Dropout(p=0.1, inplace=False)
                            (dropout2): Dropout(p=0.1, inplace=False)
                          )
                        )
                      )
                    )
                  )
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedActDense(
                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                  (wo): Linear(in_features=2816, out_features=1024, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): Linear(in_features=1024, out_features=1024, bias=False)
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): Linear(in_features=1024, out_features=1024, bias=False)
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedActDense(
                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                  (wo): Linear(in_features=2816, out_features=1024, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1-23): 23 x T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): Linear(in_features=1024, out_features=1024, bias=False)
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k): Linear(in_features=1024, out_features=1024, bias=False)
                  (v): Linear(
                    in_features=1024, out_features=1024, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedActDense(
                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                  (wo): Linear(in_features=2816, out_features=1024, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)
    )
  )
)), ('base_model', LoraModel(
  (model): T5ForConditionalGeneration(
    (shared): Embedding(32128, 1024)
    (encoder): T5Stack(
      (embed_tokens): Embedding(32128, 1024)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): SharedWeightTransformer(
                  (transformer_encoder_): Sequential(
                    (0): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                          )
                          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (1): Linear(in_features=1024, out_features=512, bias=True)
                    (2): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                          )
                          (linear1): Linear(in_features=512, out_features=1024, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=1024, out_features=512, bias=True)
                          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (3): Linear(in_features=512, out_features=256, bias=True)
                    (4): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                          )
                          (linear1): Linear(in_features=256, out_features=512, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=512, out_features=256, bias=True)
                          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (5): Linear(in_features=256, out_features=512, bias=True)
                    (6): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                          )
                          (linear1): Linear(in_features=512, out_features=1024, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=1024, out_features=512, bias=True)
                          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (7): Linear(in_features=512, out_features=1024, bias=True)
                    (8): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                          )
                          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                  )
                )
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): SharedWeightTransformer(
                  (transformer_encoder_): Sequential(
                    (0): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                          )
                          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (1): Linear(in_features=1024, out_features=512, bias=True)
                    (2): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                          )
                          (linear1): Linear(in_features=512, out_features=1024, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=1024, out_features=512, bias=True)
                          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (3): Linear(in_features=512, out_features=256, bias=True)
                    (4): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                          )
                          (linear1): Linear(in_features=256, out_features=512, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=512, out_features=256, bias=True)
                          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (5): Linear(in_features=256, out_features=512, bias=True)
                    (6): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                          )
                          (linear1): Linear(in_features=512, out_features=1024, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=1024, out_features=512, bias=True)
                          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                    (7): Linear(in_features=512, out_features=1024, bias=True)
                    (8): TransformerEncoder(
                      (layers): ModuleList(
                        (0): TransformerEncoderLayer(
                          (self_attn): MultiheadAttention(
                            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                          )
                          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                          (dropout): Dropout(p=0.1, inplace=False)
                          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                          (dropout1): Dropout(p=0.1, inplace=False)
                          (dropout2): Dropout(p=0.1, inplace=False)
                        )
                      )
                    )
                  )
                )
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): T5Stack(
      (embed_tokens): Embedding(32128, 1024)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(
                  in_features=1024, out_features=1024, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.1, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=1024, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (lm_head): Linear(in_features=1024, out_features=32128, bias=False)
  )
)), ('base_model.model', T5ForConditionalGeneration(
  (shared): Embedding(32128, 1024)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): SharedWeightTransformer(
                (transformer_encoder_): Sequential(
                  (0): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                        )
                        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                        )
                        (linear1): Linear(in_features=512, out_features=1024, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=1024, out_features=512, bias=True)
                        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (3): Linear(in_features=512, out_features=256, bias=True)
                  (4): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                        )
                        (linear1): Linear(in_features=256, out_features=512, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=512, out_features=256, bias=True)
                        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (5): Linear(in_features=256, out_features=512, bias=True)
                  (6): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                        )
                        (linear1): Linear(in_features=512, out_features=1024, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=1024, out_features=512, bias=True)
                        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (7): Linear(in_features=512, out_features=1024, bias=True)
                  (8): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                        )
                        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                )
              )
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-23): 23 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): SharedWeightTransformer(
                (transformer_encoder_): Sequential(
                  (0): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                        )
                        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                        )
                        (linear1): Linear(in_features=512, out_features=1024, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=1024, out_features=512, bias=True)
                        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (3): Linear(in_features=512, out_features=256, bias=True)
                  (4): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                        )
                        (linear1): Linear(in_features=256, out_features=512, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=512, out_features=256, bias=True)
                        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (5): Linear(in_features=256, out_features=512, bias=True)
                  (6): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                        )
                        (linear1): Linear(in_features=512, out_features=1024, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=1024, out_features=512, bias=True)
                        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                  (7): Linear(in_features=512, out_features=1024, bias=True)
                  (8): TransformerEncoder(
                    (layers): ModuleList(
                      (0): TransformerEncoderLayer(
                        (self_attn): MultiheadAttention(
                          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                        )
                        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                        (dropout): Dropout(p=0.1, inplace=False)
                        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                        (dropout1): Dropout(p=0.1, inplace=False)
                        (dropout2): Dropout(p=0.1, inplace=False)
                      )
                    )
                  )
                )
              )
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-23): 23 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(
                in_features=1024, out_features=1024, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1024, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)
)), ('base_model.model.shared', Embedding(32128, 1024)), ('base_model.model.encoder', T5Stack(
  (embed_tokens): Embedding(32128, 1024)
  (block): ModuleList(
    (0): T5Block(
      (layer): ModuleList(
        (0): T5LayerSelfAttention(
          (SelfAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): SharedWeightTransformer(
              (transformer_encoder_): Sequential(
                (0): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                      )
                      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (1): Linear(in_features=1024, out_features=512, bias=True)
                (2): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                      )
                      (linear1): Linear(in_features=512, out_features=1024, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=1024, out_features=512, bias=True)
                      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                      )
                      (linear1): Linear(in_features=256, out_features=512, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=512, out_features=256, bias=True)
                      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (5): Linear(in_features=256, out_features=512, bias=True)
                (6): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                      )
                      (linear1): Linear(in_features=512, out_features=1024, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=1024, out_features=512, bias=True)
                      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (7): Linear(in_features=512, out_features=1024, bias=True)
                (8): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                      )
                      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
              )
            )
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
            (relative_attention_bias): Embedding(32, 16)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): T5LayerFF(
          (DenseReluDense): T5DenseGatedActDense(
            (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
            (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
            (wo): Linear(in_features=2816, out_features=1024, bias=False)
            (dropout): Dropout(p=0.1, inplace=False)
            (act): NewGELUActivation()
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1-23): 23 x T5Block(
      (layer): ModuleList(
        (0): T5LayerSelfAttention(
          (SelfAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): SharedWeightTransformer(
              (transformer_encoder_): Sequential(
                (0): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                      )
                      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (1): Linear(in_features=1024, out_features=512, bias=True)
                (2): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                      )
                      (linear1): Linear(in_features=512, out_features=1024, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=1024, out_features=512, bias=True)
                      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                      )
                      (linear1): Linear(in_features=256, out_features=512, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=512, out_features=256, bias=True)
                      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (5): Linear(in_features=256, out_features=512, bias=True)
                (6): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                      )
                      (linear1): Linear(in_features=512, out_features=1024, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=1024, out_features=512, bias=True)
                      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
                (7): Linear(in_features=512, out_features=1024, bias=True)
                (8): TransformerEncoder(
                  (layers): ModuleList(
                    (0): TransformerEncoderLayer(
                      (self_attn): MultiheadAttention(
                        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                      )
                      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                      (dropout): Dropout(p=0.1, inplace=False)
                      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                      (dropout1): Dropout(p=0.1, inplace=False)
                      (dropout2): Dropout(p=0.1, inplace=False)
                    )
                  )
                )
              )
            )
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): T5LayerFF(
          (DenseReluDense): T5DenseGatedActDense(
            (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
            (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
            (wo): Linear(in_features=2816, out_features=1024, bias=False)
            (dropout): Dropout(p=0.1, inplace=False)
            (act): NewGELUActivation()
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (final_layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block', ModuleList(
  (0): T5Block(
    (layer): ModuleList(
      (0): T5LayerSelfAttention(
        (SelfAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): SharedWeightTransformer(
            (transformer_encoder_): Sequential(
              (0): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                    )
                    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (1): Linear(in_features=1024, out_features=512, bias=True)
              (2): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                    )
                    (linear1): Linear(in_features=512, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=1024, out_features=512, bias=True)
                    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (3): Linear(in_features=512, out_features=256, bias=True)
              (4): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                    )
                    (linear1): Linear(in_features=256, out_features=512, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=512, out_features=256, bias=True)
                    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (5): Linear(in_features=256, out_features=512, bias=True)
              (6): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                    )
                    (linear1): Linear(in_features=512, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=1024, out_features=512, bias=True)
                    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (7): Linear(in_features=512, out_features=1024, bias=True)
              (8): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                    )
                    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
          (relative_attention_bias): Embedding(32, 16)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): T5LayerFF(
        (DenseReluDense): T5DenseGatedActDense(
          (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
          (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
          (wo): Linear(in_features=2816, out_features=1024, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
          (act): NewGELUActivation()
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1-23): 23 x T5Block(
    (layer): ModuleList(
      (0): T5LayerSelfAttention(
        (SelfAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): SharedWeightTransformer(
            (transformer_encoder_): Sequential(
              (0): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                    )
                    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (1): Linear(in_features=1024, out_features=512, bias=True)
              (2): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                    )
                    (linear1): Linear(in_features=512, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=1024, out_features=512, bias=True)
                    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (3): Linear(in_features=512, out_features=256, bias=True)
              (4): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                    )
                    (linear1): Linear(in_features=256, out_features=512, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=512, out_features=256, bias=True)
                    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (5): Linear(in_features=256, out_features=512, bias=True)
              (6): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                    )
                    (linear1): Linear(in_features=512, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=1024, out_features=512, bias=True)
                    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
              (7): Linear(in_features=512, out_features=1024, bias=True)
              (8): TransformerEncoder(
                (layers): ModuleList(
                  (0): TransformerEncoderLayer(
                    (self_attn): MultiheadAttention(
                      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                    )
                    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (dropout1): Dropout(p=0.1, inplace=False)
                    (dropout2): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): T5LayerFF(
        (DenseReluDense): T5DenseGatedActDense(
          (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
          (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
          (wo): Linear(in_features=2816, out_features=1024, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
          (act): NewGELUActivation()
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
)), ('base_model.model.encoder.block.0', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
        (relative_attention_bias): Embedding(32, 16)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
      (relative_attention_bias): Embedding(32, 16)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
    (relative_attention_bias): Embedding(32, 16)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
  (relative_attention_bias): Embedding(32, 16)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_', Sequential(
  (0): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): Linear(in_features=1024, out_features=512, bias=True)
  (2): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (3): Linear(in_features=512, out_features=256, bias=True)
  (4): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (5): Linear(in_features=256, out_features=512, bias=True)
  (6): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (7): Linear(in_features=512, out_features=1024, bias=True)
  (8): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=1024, bias=True)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers', ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0', TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
  )
  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.self_attn', MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.self_attn.out_proj', NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.linear1', Linear(in_features=1024, out_features=2048, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.linear2', Linear(in_features=2048, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.norm1', LayerNorm((1024,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.norm2', LayerNorm((1024,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.dropout1', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.0.layers.0.dropout2', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.1', Linear(in_features=1024, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=1024, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=1024, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers', ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (linear1): Linear(in_features=512, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=512, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0', TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=1024, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=1024, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.self_attn', MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.self_attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.linear1', Linear(in_features=512, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.linear2', Linear(in_features=1024, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.norm1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.norm2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.dropout1', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.2.layers.0.dropout2', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.3', Linear(in_features=512, out_features=256, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=512, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=512, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers', ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=512, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0', TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
  )
  (linear1): Linear(in_features=256, out_features=512, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=True)
  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.self_attn', MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.self_attn.out_proj', NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.linear1', Linear(in_features=256, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.linear2', Linear(in_features=512, out_features=256, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.norm1', LayerNorm((256,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.norm2', LayerNorm((256,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.dropout1', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.4.layers.0.dropout2', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.5', Linear(in_features=256, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=1024, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=1024, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers', ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (linear1): Linear(in_features=512, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=512, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0', TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=1024, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=1024, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.self_attn', MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.self_attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.linear1', Linear(in_features=512, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.linear2', Linear(in_features=1024, out_features=512, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.norm1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.norm2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.dropout1', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.6.layers.0.dropout2', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.7', Linear(in_features=512, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=2048, out_features=1024, bias=True)
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers', ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=2048, out_features=1024, bias=True)
    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0', TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
  )
  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.self_attn', MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.self_attn.out_proj', NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.linear1', Linear(in_features=1024, out_features=2048, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.linear2', Linear(in_features=2048, out_features=1024, bias=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.norm1', LayerNorm((1024,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.norm2', LayerNorm((1024,), eps=1e-05, elementwise_affine=True)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.dropout1', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.k.transformer_encoder_.8.layers.0.dropout2', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.0.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias', Embedding(32, 16)), ('base_model.model.encoder.block.0.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.0.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.0.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.0.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.0.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.1', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.1.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.1.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.1.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.1.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.1.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.1.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.1.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.1.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.1.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.1.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.2', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.2.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.2.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.2.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.2.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.2.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.2.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.2.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.2.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.2.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.2.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.3', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.3.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.3.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.3.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.3.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.3.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.3.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.3.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.3.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.3.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.3.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.4', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.4.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.4.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.4.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.4.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.4.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.4.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.4.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.4.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.4.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.4.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.5', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.5.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.5.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.5.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.5.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.5.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.5.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.5.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.5.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.5.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.5.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.6', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.6.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.6.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.6.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.6.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.6.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.6.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.6.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.6.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.6.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.6.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.7', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.7.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.7.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.7.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.7.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.7.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.7.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.7.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.7.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.7.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.7.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.8', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.8.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.8.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.8.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.8.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.8.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.8.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.8.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.8.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.8.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.8.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.9', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.9.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.9.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.9.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.9.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.9.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.9.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.9.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.9.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.9.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.9.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.10', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.10.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.10.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.10.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.10.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.10.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.10.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.10.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.10.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.10.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.10.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.11', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.11.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.11.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.11.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.11.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.11.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.11.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.11.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.11.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.11.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.11.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.12', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.12.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.12.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.12.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.12.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.12.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.12.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.12.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.12.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.12.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.12.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.13', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.13.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.13.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.13.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.13.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.13.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.13.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.13.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.13.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.13.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.13.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.14', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.14.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.14.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.14.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.14.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.14.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.14.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.14.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.14.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.14.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.14.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.15', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.15.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.15.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.15.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.15.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.15.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.15.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.15.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.15.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.15.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.15.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.16', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.16.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.16.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.16.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.16.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.16.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.16.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.16.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.16.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.16.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.16.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.17', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.17.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.17.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.17.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.17.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.17.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.17.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.17.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.17.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.17.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.17.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.18', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.18.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.18.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.18.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.18.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.18.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.18.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.18.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.18.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.18.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.18.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.19', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.19.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.19.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.19.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.19.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.19.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.19.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.19.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.19.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.19.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.19.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.20', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.20.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.20.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.20.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.20.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.20.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.20.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.20.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.20.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.20.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.20.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.21', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.21.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.21.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.21.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.21.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.21.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.21.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.21.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.21.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.21.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.21.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.22', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.22.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.22.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.22.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.22.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.22.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.22.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.22.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.22.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.22.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.22.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.23', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): SharedWeightTransformer(
          (transformer_encoder_): Sequential(
            (0): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (1): Linear(in_features=1024, out_features=512, bias=True)
            (2): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (3): Linear(in_features=512, out_features=256, bias=True)
            (4): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (linear1): Linear(in_features=256, out_features=512, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=512, out_features=256, bias=True)
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (5): Linear(in_features=256, out_features=512, bias=True)
            (6): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=1024, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (7): Linear(in_features=512, out_features=1024, bias=True)
            (8): TransformerEncoder(
              (layers): ModuleList(
                (0): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                  )
                  (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.encoder.block.23.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): SharedWeightTransformer(
        (transformer_encoder_): Sequential(
          (0): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): Linear(in_features=1024, out_features=512, bias=True)
          (2): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): Linear(in_features=512, out_features=256, bias=True)
          (4): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): Linear(in_features=256, out_features=512, bias=True)
          (6): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): Linear(in_features=512, out_features=1024, bias=True)
          (8): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.encoder.block.23.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): SharedWeightTransformer(
      (transformer_encoder_): Sequential(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Linear(in_features=1024, out_features=512, bias=True)
        (2): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Linear(in_features=512, out_features=256, bias=True)
        (4): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=512, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): Linear(in_features=256, out_features=512, bias=True)
        (6): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): Linear(in_features=512, out_features=1024, bias=True)
        (8): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (linear1): Linear(in_features=1024, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=1024, bias=True)
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): SharedWeightTransformer(
    (transformer_encoder_): Sequential(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): Linear(in_features=1024, out_features=512, bias=True)
      (2): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=512, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): Linear(in_features=256, out_features=512, bias=True)
      (6): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=1024, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (linear1): Linear(in_features=1024, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=1024, bias=True)
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.23.layer.0.SelfAttention.k', SharedWeightTransformer(
  (transformer_encoder_): Sequential(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (3): Linear(in_features=512, out_features=256, bias=True)
    (4): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (5): Linear(in_features=256, out_features=512, bias=True)
    (6): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (7): Linear(in_features=512, out_features=1024, bias=True)
    (8): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.encoder.block.23.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.encoder.block.23.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.23.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.23.layer.1', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.block.23.layer.1.DenseReluDense.act', NewGELUActivation()), ('base_model.model.encoder.block.23.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.encoder.block.23.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.encoder.final_layer_norm', T5LayerNorm()), ('base_model.model.encoder.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder', T5Stack(
  (embed_tokens): Embedding(32128, 1024)
  (block): ModuleList(
    (0): T5Block(
      (layer): ModuleList(
        (0): T5LayerSelfAttention(
          (SelfAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): Linear(in_features=1024, out_features=1024, bias=False)
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
            (relative_attention_bias): Embedding(32, 16)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): T5LayerCrossAttention(
          (EncDecAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): Linear(in_features=1024, out_features=1024, bias=False)
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): T5LayerFF(
          (DenseReluDense): T5DenseGatedActDense(
            (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
            (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
            (wo): Linear(in_features=2816, out_features=1024, bias=False)
            (dropout): Dropout(p=0.1, inplace=False)
            (act): NewGELUActivation()
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (1-23): 23 x T5Block(
      (layer): ModuleList(
        (0): T5LayerSelfAttention(
          (SelfAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): Linear(in_features=1024, out_features=1024, bias=False)
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): T5LayerCrossAttention(
          (EncDecAttention): T5Attention(
            (q): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (k): Linear(in_features=1024, out_features=1024, bias=False)
            (v): Linear(
              in_features=1024, out_features=1024, bias=False
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1024, out_features=8, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=8, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
            )
            (o): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): T5LayerFF(
          (DenseReluDense): T5DenseGatedActDense(
            (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
            (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
            (wo): Linear(in_features=2816, out_features=1024, bias=False)
            (dropout): Dropout(p=0.1, inplace=False)
            (act): NewGELUActivation()
          )
          (layer_norm): T5LayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (final_layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block', ModuleList(
  (0): T5Block(
    (layer): ModuleList(
      (0): T5LayerSelfAttention(
        (SelfAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): Linear(in_features=1024, out_features=1024, bias=False)
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
          (relative_attention_bias): Embedding(32, 16)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): T5LayerCrossAttention(
        (EncDecAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): Linear(in_features=1024, out_features=1024, bias=False)
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): T5LayerFF(
        (DenseReluDense): T5DenseGatedActDense(
          (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
          (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
          (wo): Linear(in_features=2816, out_features=1024, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
          (act): NewGELUActivation()
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1-23): 23 x T5Block(
    (layer): ModuleList(
      (0): T5LayerSelfAttention(
        (SelfAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): Linear(in_features=1024, out_features=1024, bias=False)
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): T5LayerCrossAttention(
        (EncDecAttention): T5Attention(
          (q): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (k): Linear(in_features=1024, out_features=1024, bias=False)
          (v): Linear(
            in_features=1024, out_features=1024, bias=False
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=1024, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=1024, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (o): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): T5LayerFF(
        (DenseReluDense): T5DenseGatedActDense(
          (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
          (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
          (wo): Linear(in_features=2816, out_features=1024, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
          (act): NewGELUActivation()
        )
        (layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
)), ('base_model.model.decoder.block.0', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
        (relative_attention_bias): Embedding(32, 16)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.0.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
      (relative_attention_bias): Embedding(32, 16)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.0.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
    (relative_attention_bias): Embedding(32, 16)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
  (relative_attention_bias): Embedding(32, 16)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.0.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.0.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias', Embedding(32, 16)), ('base_model.model.decoder.block.0.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.0.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.0.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.0.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.0.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.0.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.0.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.1.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.1.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.1.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.1.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.1.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.1.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.1.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.1.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.1.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.1.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.2.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.2.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.2.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.2.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.2.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.2.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.2.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.2.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.2.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.2.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.3.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.3.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.3.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.3.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.3.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.3.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.3.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.3.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.3.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.3.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.4.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.4.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.4.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.4.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.4.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.4.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.4.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.4.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.4.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.4.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.5.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.5.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.5.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.5.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.5.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.5.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.5.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.5.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.5.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.5.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.6.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.6.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.6.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.6.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.6.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.6.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.6.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.6.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.6.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.6.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.7.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.7.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.7.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.7.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.7.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.7.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.7.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.7.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.7.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.7.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.8.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.8.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.8.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.8.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.8.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.8.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.8.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.8.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.8.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.8.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.9.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.9.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.9.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.9.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.9.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.9.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.9.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.9.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.9.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.9.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.10.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.10.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.10.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.10.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.10.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.10.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.10.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.10.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.10.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.10.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.11.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.11.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.11.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.11.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.11.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.11.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.11.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.11.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.11.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.11.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.12.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.12.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.12.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.12.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.12.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.12.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.12.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.12.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.12.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.12.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.13.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.13.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.13.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.13.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.13.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.13.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.13.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.13.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.13.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.13.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.14.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.14.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.14.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.14.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.14.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.14.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.14.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.14.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.14.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.14.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.15.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.15.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.15.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.15.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.15.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.15.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.15.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.15.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.15.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.15.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.16.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.16.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.16.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.16.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.16.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.16.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.16.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.16.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.16.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.16.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.17.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.17.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.17.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.17.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.17.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.17.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.17.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.17.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.17.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.17.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.18.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.18.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.18.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.18.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.18.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.18.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.18.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.18.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.18.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.18.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.19.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.19.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.19.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.19.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.19.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.19.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.19.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.19.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.19.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.19.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.20.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.20.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.20.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.20.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.20.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.20.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.20.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.20.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.20.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.20.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.21.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.21.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.21.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.21.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.21.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.21.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.21.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.21.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.21.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.21.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.22.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.22.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.22.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.22.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.22.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.22.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.22.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.22.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.22.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.22.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23', T5Block(
  (layer): ModuleList(
    (0): T5LayerSelfAttention(
      (SelfAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): T5LayerCrossAttention(
      (EncDecAttention): T5Attention(
        (q): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (k): Linear(in_features=1024, out_features=1024, bias=False)
        (v): Linear(
          in_features=1024, out_features=1024, bias=False
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1024, out_features=8, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=8, out_features=1024, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
        )
        (o): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): T5LayerFF(
      (DenseReluDense): T5DenseGatedActDense(
        (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
        (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
        (wo): Linear(in_features=2816, out_features=1024, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (act): NewGELUActivation()
      )
      (layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
)), ('base_model.model.decoder.block.23.layer', ModuleList(
  (0): T5LayerSelfAttention(
    (SelfAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): T5LayerCrossAttention(
    (EncDecAttention): T5Attention(
      (q): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (k): Linear(in_features=1024, out_features=1024, bias=False)
      (v): Linear(
        in_features=1024, out_features=1024, bias=False
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=1024, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=1024, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (o): Linear(in_features=1024, out_features=1024, bias=False)
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (2): T5LayerFF(
    (DenseReluDense): T5DenseGatedActDense(
      (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
      (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
      (wo): Linear(in_features=2816, out_features=1024, bias=False)
      (dropout): Dropout(p=0.1, inplace=False)
      (act): NewGELUActivation()
    )
    (layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)), ('base_model.model.decoder.block.23.layer.0', T5LayerSelfAttention(
  (SelfAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.23.layer.0.SelfAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.23.layer.0.SelfAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.0.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.23.layer.0.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.1', T5LayerCrossAttention(
  (EncDecAttention): T5Attention(
    (q): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (k): Linear(in_features=1024, out_features=1024, bias=False)
    (v): Linear(
      in_features=1024, out_features=1024, bias=False
      (lora_dropout): ModuleDict(
        (default): Dropout(p=0.1, inplace=False)
      )
      (lora_A): ModuleDict(
        (default): Linear(in_features=1024, out_features=8, bias=False)
      )
      (lora_B): ModuleDict(
        (default): Linear(in_features=8, out_features=1024, bias=False)
      )
      (lora_embedding_A): ParameterDict()
      (lora_embedding_B): ParameterDict()
    )
    (o): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention', T5Attention(
  (q): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (k): Linear(in_features=1024, out_features=1024, bias=False)
  (v): Linear(
    in_features=1024, out_features=1024, bias=False
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=1024, out_features=8, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=8, out_features=1024, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (o): Linear(in_features=1024, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.k', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v', Linear(
  in_features=1024, out_features=1024, bias=False
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.1, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=1024, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=1024, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_dropout', ModuleDict(
  (default): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_dropout.default', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A', ModuleDict(
  (default): Linear(in_features=1024, out_features=8, bias=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.default', Linear(in_features=1024, out_features=8, bias=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B', ModuleDict(
  (default): Linear(in_features=8, out_features=1024, bias=False)
)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.default', Linear(in_features=8, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_embedding_A', ParameterDict()), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_embedding_B', ParameterDict()), ('base_model.model.decoder.block.23.layer.1.EncDecAttention.o', Linear(in_features=1024, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.1.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.23.layer.1.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.2', T5LayerFF(
  (DenseReluDense): T5DenseGatedActDense(
    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
    (wo): Linear(in_features=2816, out_features=1024, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
    (act): NewGELUActivation()
  )
  (layer_norm): T5LayerNorm()
  (dropout): Dropout(p=0.1, inplace=False)
)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense', T5DenseGatedActDense(
  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
  (wo): Linear(in_features=2816, out_features=1024, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
  (act): NewGELUActivation()
)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense.wi_0', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense.wi_1', Linear(in_features=1024, out_features=2816, bias=False)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense.wo', Linear(in_features=2816, out_features=1024, bias=False)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.block.23.layer.2.DenseReluDense.act', NewGELUActivation()), ('base_model.model.decoder.block.23.layer.2.layer_norm', T5LayerNorm()), ('base_model.model.decoder.block.23.layer.2.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.decoder.final_layer_norm', T5LayerNorm()), ('base_model.model.decoder.dropout', Dropout(p=0.1, inplace=False)), ('base_model.model.lm_head', Linear(in_features=1024, out_features=32128, bias=False))]
trainable params: 25,204,736 || all params: 808,354,816 || trainable%: 3.11802880382666
